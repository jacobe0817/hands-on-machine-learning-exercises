{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91e2f91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier, SGDRegressor\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "rng = np.random.default_rng(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11be9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Train a LinearSVC on a linearly separable dataset. Then train an SVC and a SGDClassifier on the same dataset.\n",
    "#    See if you can get them to produce roughly the same model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11262b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a linearly separable dataset\n",
    "# for this example, I will create a positive class (y = 1) centered around (1, 1)\n",
    "# and a negative class (y = 0) centered around (-1, -1)\n",
    "# each instance has random uniform error, on each axis, in the interval [-.5, .5)\n",
    "# each class has 100 instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431a3bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_positive = rng.random((100, 2)) + .5\n",
    "y_positive = np.full((100), 1)\n",
    "\n",
    "X_negative = -1 * rng.random((100, 2)) - .5\n",
    "y_negative = np.full((100), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009519a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# brief validation to show that there are no values in either column of X_positive that are negative\n",
    "# and there are no values in either column of X_negative that are positive\n",
    "# which guarantees that the dataset is linearly separable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d71471b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_positive[(X_positive[:, 0] < 0) | (X_positive[:, 1] < 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23aa7076",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_negative[(X_negative[:, 0] > 0) | (X_negative[:, 1] > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1664eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# brief validation to show that X_positive and X_negative are centered around (1, 1) and (-1, -1), respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e2524a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_positive.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f012380",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_negative.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df4e7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# glue the datasets together, then shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593cc0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.append(X_positive, X_negative, axis=0)\n",
    "y = np.append(y_positive, y_negative, axis=0)\n",
    "\n",
    "X, y = shuffle(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd48b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# briefly verify that training instances and labels were shuffled correctly\n",
    "# i.e. positive instances assigned to positive class and negative to negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbee3a9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "((np.apply_along_axis(sum, 1, X) >= 0).astype(int) == y)[False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724ad02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normally I would create a test set and perform cross-validation but for this example I only care about\n",
    "# the similarity of the trained models rather than their performance and generalizability.\n",
    "# and besides, I know the exact logic of how the dataset was created because I created it,\n",
    "# which is the ultimate form of data snooping\n",
    "\n",
    "# so I will just train the models and see how similar I can make them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb97a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard scale features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61091041",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "X_scaled = ss.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82624de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab539e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_svc = LinearSVC(loss='hinge')\n",
    "linear_svc.fit(X_scaled, y)\n",
    "print(f'intercept : {linear_svc.intercept_} \\nweights : {linear_svc.coef_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98146086",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(kernel='linear')\n",
    "svc.fit(X_scaled, y)\n",
    "print(f'intercept : {svc.intercept_} \\nweights : {svc.coef_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67916b77",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sgd_classifier = SGDClassifier(alpha=.005, tol=.0001, max_iter=1_000_000, n_iter_no_change=100_000, random_state=42)\n",
    "sgd_classifier.fit(X_scaled, y)\n",
    "print(f'intercept : {sgd_classifier.intercept_} \\nweights : {sgd_classifier.coef_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebd3cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "924e5899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Train an SVM classifier on the MNIST dataset. Since SVM classifiers are binary classifiers,\n",
    "#    you will need to use one-versus-the-rest to classify all 10 digits. You may want to tune the\n",
    "#    hyperparameters using small validation sets speed up the process. What accuracy can you reach?\n",
    "\n",
    "# note that for LinearSVC there is a parameter for multi_class='ovr', but I will implement this myself for practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90c816c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97eee781",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "\n",
    "features = mnist['data']\n",
    "\n",
    "labels = mnist['target']\n",
    "labels = labels.astype(np.uint16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "873814f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and test set\n",
    "# NOTE: data is preshuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b1b9b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = features[:60000]\n",
    "train_labels = labels[:60000]\n",
    "\n",
    "test_features = features[60000:]\n",
    "test_labels = labels[60000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04a28194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "262fb9a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c3a80a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data preparer and prep data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75a38446",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_scaler = StandardScaler()\n",
    "prepped_train_features = mnist_scaler.fit_transform(train_features)\n",
    "prepped_test_features = mnist_scaler.transform(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92825e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create label matrix preparer\n",
    "\n",
    "# takes label vector and turns it into a matrix where each column corresponds to a class (digit) and each row is an instance,\n",
    "# where a 1 indicates the instance belongs to that class and a 0 indicates that it does not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb44fa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelMatrixPreparer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    def fit(self, X):\n",
    "        self.classes_ = np.unique(X)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        label_matrix = None\n",
    "        for class_ in self.classes_:\n",
    "            new_column = (X == class_).astype(int)\n",
    "            if label_matrix is None:\n",
    "                label_matrix = new_column\n",
    "            else:\n",
    "                label_matrix = np.c_[label_matrix, new_column]\n",
    "        return label_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b224a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quickly validate label matrix preparer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b81d5641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: class, dtype: bool)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmp = LabelMatrixPreparer()\n",
    "train_label_matrix = lmp.fit_transform(train_labels)\n",
    "validation_array = train_label_matrix[:, 4] == (train_labels == 4)\n",
    "validation_array[validation_array == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3fdfbf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create OVR estimator that trains one SVM for each class according to corresponding column in label matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7f800c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OVR_SVM_Estimator(BaseEstimator):\n",
    "    def __init__(self, class_names, C=1):\n",
    "        self.class_names = class_names\n",
    "        self.C = C\n",
    "        return\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.estimators_ = []\n",
    "        for col_idx in range(y.shape[1]):\n",
    "            estimator = LinearSVC(C=self.C, max_iter=1_000_000)\n",
    "            estimator.fit(X, y[:, col_idx])\n",
    "            self.estimators_.append(estimator)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        decision_scores = None\n",
    "        for estimator in self.estimators_:\n",
    "            new_column = estimator.decision_function(X)\n",
    "            if decision_scores is None:\n",
    "                decision_scores = new_column\n",
    "            else:\n",
    "                decision_scores = np.c_[decision_scores, new_column]\n",
    "        \n",
    "        #self.class_names[arg_max]\n",
    "        return decision_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cebd5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "ovr = OVR_SVM_Estimator(list(range(10)))\n",
    "ovr.fit(prepped_train_features, train_label_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2605627",
   "metadata": {},
   "outputs": [],
   "source": [
    "ovr.predict(prepped_train_features)[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945643c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ef4776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform gridsearch on OVR estimator to select hyperparameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30e4667",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c5b1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0b91dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88422182",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74391ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Train an SVM regressor on the California housing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3147e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "921daf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('housing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84762d1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "      <th>ocean_proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.23</td>\n",
       "      <td>37.88</td>\n",
       "      <td>41.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>8.3252</td>\n",
       "      <td>452600.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-122.22</td>\n",
       "      <td>37.86</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7099.0</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>8.3014</td>\n",
       "      <td>358500.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-122.24</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1467.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>7.2574</td>\n",
       "      <td>352100.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1274.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>558.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>5.6431</td>\n",
       "      <td>341300.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1627.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>3.8462</td>\n",
       "      <td>342200.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0    -122.23     37.88                41.0        880.0           129.0   \n",
       "1    -122.22     37.86                21.0       7099.0          1106.0   \n",
       "2    -122.24     37.85                52.0       1467.0           190.0   \n",
       "3    -122.25     37.85                52.0       1274.0           235.0   \n",
       "4    -122.25     37.85                52.0       1627.0           280.0   \n",
       "\n",
       "   population  households  median_income  median_house_value ocean_proximity  \n",
       "0       322.0       126.0         8.3252            452600.0        NEAR BAY  \n",
       "1      2401.0      1138.0         8.3014            358500.0        NEAR BAY  \n",
       "2       496.0       177.0         7.2574            352100.0        NEAR BAY  \n",
       "3       558.0       219.0         5.6431            341300.0        NEAR BAY  \n",
       "4       565.0       259.0         3.8462            342200.0        NEAR BAY  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3423f42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column to raw_data that assigns which decile of median_house_value that instance belongs to\n",
    "\n",
    "# note that this column is created to allow stratified sampling of the target in the creation of our test set.\n",
    "# because the deciles are calculated on the entire dataset (train and test),\n",
    "# it is important that we drop the column as soon as the train/test sets are formed.\n",
    "# this column should NOT be treated as an input feature because, having been calculated on the full dataset,\n",
    "# it is data snooping and will likely not generalize well to new data if used to train a model.\n",
    "# the column exists purely to ensure a wide variety of targets exists in both the train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4df3b071",
   "metadata": {},
   "outputs": [],
   "source": [
    "decile_1 = raw_data['median_house_value'].quantile(q=.1)\n",
    "decile_2 = raw_data['median_house_value'].quantile(q=.2)\n",
    "decile_3 = raw_data['median_house_value'].quantile(q=.3)\n",
    "decile_4 = raw_data['median_house_value'].quantile(q=.4)\n",
    "decile_5 = raw_data['median_house_value'].quantile(q=.5)\n",
    "decile_6 = raw_data['median_house_value'].quantile(q=.6)\n",
    "decile_7 = raw_data['median_house_value'].quantile(q=.7)\n",
    "decile_8 = raw_data['median_house_value'].quantile(q=.8)\n",
    "decile_9 = raw_data['median_house_value'].quantile(q=.9)\n",
    "\n",
    "decile_bins = [-np.inf, decile_1, decile_2, decile_3, decile_4, decile_5, decile_6, decile_7, decile_8, decile_9, np.inf]\n",
    "\n",
    "raw_data['decile'] = pd.cut(raw_data['median_house_value'], bins=decile_bins, labels=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1324f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training and test sets, drop decile column, and separate into features and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64c1fc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stratified_splitter = StratifiedShuffleSplit(n_splits=1, test_size=.2, random_state=42)\n",
    "train_test_split = stratified_splitter.split(raw_data, raw_data['decile'])\n",
    "\n",
    "train_set = None\n",
    "test_set = None\n",
    "\n",
    "for train_indices, test_indices in train_test_split:\n",
    "    train_set = raw_data.loc[train_indices]\n",
    "    test_set = raw_data.loc[test_indices]\n",
    "\n",
    "train_targets = train_set['median_house_value'].copy()\n",
    "test_targets = train_set['median_house_value'].copy()\n",
    "\n",
    "train_features = train_set.drop(columns=['median_house_value', 'decile'])\n",
    "test_features = test_set.drop(columns=['median_house_value', 'decile'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49e3b7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create variables for column names and identify indices for columns used in added numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d96b38b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rooms_string = 'total_rooms'\n",
    "bedrooms_string = 'total_bedrooms'\n",
    "population_string = 'population'\n",
    "households_string = 'households'\n",
    "ocean_proximity_string = 'ocean_proximity'\n",
    "\n",
    "train_features_column_names = list(train_features.columns)\n",
    "\n",
    "rooms_index = train_features_column_names.index(rooms_string)\n",
    "bedrooms_index = train_features_column_names.index(bedrooms_string)\n",
    "population_index = train_features_column_names.index(population_string)\n",
    "households_index = train_features_column_names.index(households_string)\n",
    "\n",
    "bpr_string = 'bedrooms_per_room'\n",
    "rph_string = 'rooms_per_household'\n",
    "pph_string = 'population_per_household'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aec5e295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a transformer that adds the extra numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56da4095",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumericalFeatureAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, numerical_features, add_bedrooms_per_room=False, add_rooms_per_household=False, add_population_per_household=False):\n",
    "        self.numerical_features = numerical_features\n",
    "        \n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "        self.add_rooms_per_household = add_rooms_per_household\n",
    "        self.add_population_per_household = add_population_per_household\n",
    "        return\n",
    "    \n",
    "    def fit(self, X):\n",
    "        possible_features = (bpr_string, rph_string, pph_string)\n",
    "        parameter_settings = (self.add_bedrooms_per_room, self.add_rooms_per_household, self.add_population_per_household)\n",
    "\n",
    "        self.possible_features_and_parameter_settings = list(zip(possible_features, parameter_settings))\n",
    "        \n",
    "        self.added_features = []\n",
    "        \n",
    "        for possible_feature, parameter_setting in self.possible_features_and_parameter_settings:\n",
    "            if parameter_setting:\n",
    "                self.added_features.append(possible_feature)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        new_columns = None\n",
    "        \n",
    "        for possible_feature, parameter_setting in self.possible_features_and_parameter_settings:\n",
    "            new_column = None\n",
    "            if parameter_setting:\n",
    "                if possible_feature == bpr_string:\n",
    "                    new_column = X[:, bedrooms_index] / X[:, rooms_index]\n",
    "                elif possible_feature == rph_string:\n",
    "                    new_column = X[:, rooms_index] / X[:, households_index]\n",
    "                elif possible_feature == pph_string:\n",
    "                    new_column = X[:, population_index] / X[:, households_index]\n",
    "                \n",
    "                if new_columns is None:\n",
    "                    new_columns = new_column\n",
    "                else:\n",
    "                    new_columns = np.c_[new_columns, new_column]\n",
    "        \n",
    "        if new_columns is None:\n",
    "            return X\n",
    "        \n",
    "        return np.c_[X, new_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d2e39bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a column transformer that performs data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e02d8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreparer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, numerical_features, categorical_features, add_bpr=False, add_rph=False, add_pph=False, polynomial_degree=1, interaction_terms_only=False):\n",
    "        self.numerical_features = numerical_features        \n",
    "        self.categorical_features = categorical_features\n",
    "        \n",
    "        self.add_bpr = add_bpr\n",
    "        self.add_rph = add_rph\n",
    "        self.add_pph = add_pph\n",
    "        self.polynomial_degree = polynomial_degree\n",
    "        self.interaction_terms_only = interaction_terms_only\n",
    "        return\n",
    "    \n",
    "    def fit(self, X):\n",
    "        numerical_pipeline = Pipeline([('imputer', SimpleImputer(strategy='median')),\n",
    "                                           ('feature_adder', NumericalFeatureAdder(self.numerical_features, add_bedrooms_per_room=self.add_bpr, add_rooms_per_household=self.add_rph, add_population_per_household=self.add_pph)),\n",
    "                                           ('polynomial_features', PolynomialFeatures(degree=self.polynomial_degree, interaction_only=self.interaction_terms_only, include_bias=False)),\n",
    "                                           ('scaler', StandardScaler())])\n",
    "        self.column_transformer = ColumnTransformer([('numerical', numerical_pipeline, self.numerical_features)\n",
    "                                                       , ('categorical', OneHotEncoder(handle_unknown='ignore'), self.categorical_features)])\n",
    "        \n",
    "        self.column_transformer.fit(X)\n",
    "        \n",
    "        feature_adder = self.column_transformer.named_transformers_['numerical'].named_steps['feature_adder']\n",
    "        self.added_numerical_features = feature_adder.added_features\n",
    "\n",
    "        polynomial_features = self.column_transformer.named_transformers_['numerical'].named_steps['polynomial_features']\n",
    "        numerical_features_plus_added_numerical_features = self.numerical_features + self.added_numerical_features\n",
    "        self.all_numerical_features = [' * '.join([f'{numerical_features_plus_added_numerical_features[feature_index]}^{power_array[feature_index]}' for feature_index in range(len(power_array)) if power_array[feature_index] != 0]) for power_array in polynomial_features.powers_]\n",
    "        # polynomial_features.get_feature_names_out(input_features=self.numerical_features + self.added_numerical_features)\n",
    "\n",
    "        one_hot_encoder = self.column_transformer.named_transformers_['categorical']\n",
    "        self.all_categorical_features = list(np.concatenate(one_hot_encoder.categories_))\n",
    "        \n",
    "        self.features = self.all_numerical_features + self.all_categorical_features\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return self.column_transformer.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25f145df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['longitude^1',\n",
       " 'latitude^1',\n",
       " 'housing_median_age^1',\n",
       " 'total_rooms^1',\n",
       " 'total_bedrooms^1',\n",
       " 'population^1',\n",
       " 'households^1',\n",
       " 'median_income^1',\n",
       " 'bedrooms_per_room^1',\n",
       " 'longitude^1 * latitude^1',\n",
       " 'longitude^1 * housing_median_age^1',\n",
       " 'longitude^1 * total_rooms^1',\n",
       " 'longitude^1 * total_bedrooms^1',\n",
       " 'longitude^1 * population^1',\n",
       " 'longitude^1 * households^1',\n",
       " 'longitude^1 * median_income^1',\n",
       " 'longitude^1 * bedrooms_per_room^1',\n",
       " 'latitude^1 * housing_median_age^1',\n",
       " 'latitude^1 * total_rooms^1',\n",
       " 'latitude^1 * total_bedrooms^1',\n",
       " 'latitude^1 * population^1',\n",
       " 'latitude^1 * households^1',\n",
       " 'latitude^1 * median_income^1',\n",
       " 'latitude^1 * bedrooms_per_room^1',\n",
       " 'housing_median_age^1 * total_rooms^1',\n",
       " 'housing_median_age^1 * total_bedrooms^1',\n",
       " 'housing_median_age^1 * population^1',\n",
       " 'housing_median_age^1 * households^1',\n",
       " 'housing_median_age^1 * median_income^1',\n",
       " 'housing_median_age^1 * bedrooms_per_room^1',\n",
       " 'total_rooms^1 * total_bedrooms^1',\n",
       " 'total_rooms^1 * population^1',\n",
       " 'total_rooms^1 * households^1',\n",
       " 'total_rooms^1 * median_income^1',\n",
       " 'total_rooms^1 * bedrooms_per_room^1',\n",
       " 'total_bedrooms^1 * population^1',\n",
       " 'total_bedrooms^1 * households^1',\n",
       " 'total_bedrooms^1 * median_income^1',\n",
       " 'total_bedrooms^1 * bedrooms_per_room^1',\n",
       " 'population^1 * households^1',\n",
       " 'population^1 * median_income^1',\n",
       " 'population^1 * bedrooms_per_room^1',\n",
       " 'households^1 * median_income^1',\n",
       " 'households^1 * bedrooms_per_room^1',\n",
       " 'median_income^1 * bedrooms_per_room^1',\n",
       " '<1H OCEAN',\n",
       " 'INLAND',\n",
       " 'ISLAND',\n",
       " 'NEAR BAY',\n",
       " 'NEAR OCEAN']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_features = [ocean_proximity_string]\n",
    "numerical_features = list(train_features.drop(columns=categorical_features))\n",
    "\n",
    "data_preparer = DataPreparer(numerical_features, categorical_features, add_bpr=True, polynomial_degree=2, interaction_terms_only=True)\n",
    "prepped_train_features = data_preparer.fit_transform(train_features)\n",
    "\n",
    "data_preparer.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c88e23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# brief validation of data preparer on first row of shuffled training features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "700abd31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16512, 9)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "529ff99a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16512, 50)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepped_train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6dda0743",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "longitude             -116.5\n",
       "latitude               33.82\n",
       "housing_median_age      16.0\n",
       "total_rooms            343.0\n",
       "total_bedrooms          85.0\n",
       "population              29.0\n",
       "households              14.0\n",
       "median_income         2.1042\n",
       "ocean_proximity       INLAND\n",
       "Name: 12344, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e25356c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.53652021, -0.8524502 , -1.00209463, -1.04711441, -1.0773835 ,\n",
       "       -1.25320775, -1.26619991, -0.92674015,  0.54898116,  1.0060471 ,\n",
       "        1.02989419,  1.05465074,  1.08641961,  1.25706504,  1.27015793,\n",
       "        0.95387955, -0.45035876, -1.04951618, -1.06151519, -1.09593943,\n",
       "       -1.26210364, -1.27462169, -0.98345352,  0.35578052, -1.41823592,\n",
       "       -1.32731384, -1.44661167, -1.45043049, -0.99668143, -0.63188836,\n",
       "       -0.34987324, -0.33080851, -0.3554938 , -0.79244735, -1.0773835 ,\n",
       "       -0.36341573, -0.38006211, -0.9136977 , -0.94142066, -0.36437014,\n",
       "       -0.97325909, -1.16058851, -0.99310491, -1.13858577, -0.91424779,\n",
       "        0.        ,  1.        ,  0.        ,  0.        ,  0.        ])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepped_train_features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2f695160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5364736790267735"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_features.iloc[0, 0] - train_features['longitude'].mean())/train_features['longitude'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6aeb9666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a data preparation and prediction pipeline to experiment with different hyperparameters in GridSearchCV\n",
    "\n",
    "# I wanted to try implementing the SVR or LinearSVR class, but they both took far too long even on a broad grid search.\n",
    "# so instead I am implementing SGDRegressor and using epsilon insensitive to capture the desired SVM behavior.\n",
    "\n",
    "# furthermore, when fitting the estimator, I will calculate the standard deviation of the training target values\n",
    "# and use this as a multiplier for the epsilon value. This will ensure the epsilon parameter is scaled appropriately\n",
    "# to the magnitude of the target values\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html\n",
    "# can probably use this to make code more concise\n",
    "# all steps in a pipeline must be transformers, except for the final step. the final step can be a estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0aee7c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreparerAndPredictor(BaseEstimator):\n",
    "    def __init__(self, numerical_features, categorical_features, add_bpr=False, add_rph=False, add_pph=False,\n",
    "                 polynomial_degree=1, interaction_terms_only=False, epsilon=0, loss='epsilon_insensitive', alpha=0,\n",
    "                 penalty='l2', l1_ratio=.15):\n",
    "        self.numerical_features = numerical_features\n",
    "        self.categorical_features = categorical_features\n",
    "        \n",
    "        self.add_bpr = add_bpr\n",
    "        self.add_rph = add_rph\n",
    "        self.add_pph = add_pph\n",
    "        self.polynomial_degree = polynomial_degree\n",
    "        self.interaction_terms_only = interaction_terms_only\n",
    "        \n",
    "        self.epsilon = epsilon\n",
    "        self.loss = loss\n",
    "        self.alpha = alpha\n",
    "        self.penalty = penalty\n",
    "        self.l1_ratio = l1_ratio\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.scaled_epsilon = self.epsilon * y.std()\n",
    "        self.data_preparer = DataPreparer(self.numerical_features, self.categorical_features, add_bpr=self.add_bpr,\n",
    "                                          add_rph=self.add_rph, add_pph=self.add_pph, polynomial_degree=self.polynomial_degree,\n",
    "                                          interaction_terms_only=self.interaction_terms_only)\n",
    "        self.predictor = SGDRegressor(epsilon=self.scaled_epsilon, loss=self.loss, alpha=self.alpha, penalty=self.penalty,\n",
    "                                      l1_ratio=self.l1_ratio, max_iter=100_000_000)\n",
    "        \n",
    "        self.data_preparer.fit(X)\n",
    "        prepped_X = self.data_preparer.transform(X)\n",
    "        self.predictor.fit(prepped_X, y)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        prepped_X = self.data_preparer.transform(X)\n",
    "        return self.predictor.predict(prepped_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1e966830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a coarse grid/random search with cv=5 to find the best hyperparameter values\n",
    "\n",
    "# make sure to return train scores so you can compare train and validation scores.\n",
    "# if there is a large discrepancy between train and validation scores, you are overfitting and should regularize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b671f975",
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse_polynomial_degree_list = [1, 2, 3]\n",
    "coarse_alpha_list = [0, 1, 10]\n",
    "coarse_epsilon_list = [0, 1, 10]\n",
    "coarse_l1_ratio_list = [.25, .5, .75]\n",
    "\n",
    "coarse_parameter_grid = [\n",
    "    {'polynomial_degree' : [2], 'interaction_terms_only' : [True], 'penalty' : ['elasticnet'], 'alpha' : coarse_alpha_list,\n",
    "     'epsilon' : coarse_epsilon_list, 'l1_ratio' : coarse_l1_ratio_list},\n",
    "    {'polynomial_degree' : coarse_polynomial_degree_list, 'interaction_terms_only' : [False], 'penalty' : ['elasticnet'], 'alpha' : coarse_alpha_list,\n",
    "     'epsilon' : coarse_epsilon_list, 'l1_ratio' : coarse_l1_ratio_list},\n",
    "    {'polynomial_degree' : [2], 'interaction_terms_only' : [True], 'penalty' : ['l2'], 'alpha' : coarse_alpha_list,\n",
    "     'epsilon' : coarse_epsilon_list},\n",
    "    {'polynomial_degree' : coarse_polynomial_degree_list, 'interaction_terms_only' : [False], 'penalty' : ['l2'], 'alpha' : coarse_alpha_list,\n",
    "     'epsilon' : coarse_epsilon_list}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0ea4369c",
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse_parameter_grid = {'penalty' : ['l2'], 'alpha' : coarse_alpha_list, 'epsilon' : coarse_epsilon_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "13ed3aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse_parameter_grid = {'alpha' : coarse_alpha_list, 'loss' : ['epsilon_insensitive'], 'max_iter' : [100_000_000], 'epsilon' : [10_000]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53be6b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse_grid_search = GridSearchCV(SGDRegressor(), coarse_parameter_grid, cv=3, scoring='neg_mean_squared_error', return_train_score=True)\n",
    "coarse_grid_search.fit(prepped_train_features, train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23426cf5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.sqrt(-coarse_grid_search.cv_results_['mean_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3ffb42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cv_results = coarse_grid_search.cv_results_\n",
    "for negative_mse, params in sorted(zip(cv_results['mean_test_score'], cv_results['params']), reverse=True, key=lambda zip_pair : zip_pair[0]):\n",
    "    print(f'{params}  ==  {np.sqrt(-negative_mse)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb3ab28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a more refined grid/random search centered on the best hyperparameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b01168",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55f68c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select model hyperparameters and train on full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a278b3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3387ad64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f21b88b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
